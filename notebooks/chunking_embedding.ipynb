{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "Splits narratives from `filtered_complaints.csv` into chunks, generates embeddings using `sentence-transformers/all-MiniLM-L6-v2`, and stores them in a FAISS vector database for Task 3 retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and load data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_file = '../data/processed/filtered_complaints.csv'\n",
    "chunks_file = '../data/processed/complaint_chunks.csv'\n",
    "\n",
    "# Verify input file\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"File not found at: {os.path.abspath(input_file)}\")\n",
    "\n",
    "# Load filtered dataset\n",
    "print(\"Loading filtered complaints...\")\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df)} complaints with columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = ['Complaint ID', 'Product', 'Consumer complaint narrative']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Chunk narratives\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Adjust based on Task 1 narrative length stats\n",
    "    chunk_overlap=50,\n",
    "    length_function=lambda x: len(x.split())\n",
    ")\n",
    "\n",
    "# Split narratives\n",
    "print(\"Chunking narratives...\")\n",
    "chunks = []\n",
    "for idx, row in df.iterrows():\n",
    "    splits = splitter.split_text(row['Consumer complaint narrative'])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            'complaint_id': row['Complaint ID'],\n",
    "            'product': row['Product'],\n",
    "            'chunk': split\n",
    "        })\n",
    "\n",
    "# Save chunks\n",
    "df_chunks = pd.DataFrame(chunks)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "df_chunks.to_csv(chunks_file, index=False)\n",
    "print(f\"Created {len(df_chunks)} chunks, saved to {chunks_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load chunks\n",
    "df_chunks = pd.read_csv(chunks_file)\n",
    "\n",
    "# Initialize model\n",
    "print(\"Generating embeddings...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df_chunks['chunk'].tolist(), batch_size=32, show_progress_bar=True)\n",
    "df_chunks['embedding'] = embeddings.tolist()\n",
    "print(f\"Generated embeddings for {len(df_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create and save FAISS index\n",
    "import faiss\n",
    "\n",
    "# Paths\n",
    "index_file = '../vector_store/faiss_index.bin'\n",
    "metadata_file = '../vector_store/metadata.csv'\n",
    "\n",
    "# Convert embeddings to numpy\n",
    "embeddings_np = np.array(df_chunks['embedding'].tolist(), dtype=np.float32)\n",
    "dimension = embeddings_np.shape[1]  # 384 for all-MiniLM-L6-v2\n",
    "\n",
    "# Create FAISS index\n",
    "print(\"Building FAISS index...\")\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Save index and metadata\n",
    "os.makedirs('../vector_store', exist_ok=True)\n",
    "faiss.write_index(index, index_file)\n",
    "df_chunks[['complaint_id', 'product', 'chunk']].to_csv(metadata_file, index=False)\n",
    "print(f\"Saved FAISS index with {index.ntotal} vectors to {index_file}\")\n",
    "print(f\"Saved metadata to {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Verify FAISS index\n",
    "index = faiss.read_index(index_file)\n",
    "print(f\"Verified FAISS index: {index.ntotal} vectors, dimension {index.d}\")\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "print(f\"Verified metadata: {len(metadata)} rows with columns {metadata.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}