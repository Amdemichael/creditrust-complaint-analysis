{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "This notebook completes Task 2 by:\n",
    "1. Loading the filtered complaints data\n",
    "2. Chunking narratives into smaller pieces\n",
    "3. Generating embeddings using sentence-transformers\n",
    "4. Creating a FAISS vector store for efficient retrieval\n",
    "5. Testing the vector store with similarity search\n",
    "6. Providing a summary for reporting\n",
    "\n",
    "**Prerequisites:** Task 1 must be completed (filtered_complaints.csv must exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered complaints...\n",
      "Loaded 355,635 complaints with columns: ['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via', 'Date sent to company', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Complaint ID']\n",
      "âœ… Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Paths\n",
    "input_file = '../data/processed/filtered_complaints.csv'\n",
    "chunks_file = '../data/processed/complaint_chunks.csv'\n",
    "\n",
    "# Verify input file\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"File not found at: {os.path.abspath(input_file)}\")\n",
    "\n",
    "# Load filtered dataset\n",
    "print(\"Loading filtered complaints...\")\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df):,} complaints with columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = ['Complaint ID', 'Product', 'Consumer complaint narrative']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chunk Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking narratives...\n",
      "âœ… Created 392,406 chunks, saved to ../data/processed/complaint_chunks.csv\n",
      "\n",
      "ðŸ“Š Chunk Statistics:\n",
      "Total chunks: 392,406\n",
      "Unique complaints: 355,635\n",
      "Average words per chunk: 184.8\n",
      "Median words per chunk: 140.0\n",
      "Products: ['Checking or savings account'\n",
      " 'Money transfer, virtual currency, or money service'\n",
      " 'Credit card or prepaid card' 'Consumer Loan']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Based on Task 1 narrative length analysis\n",
    "    chunk_overlap=50,\n",
    "    length_function=lambda x: len(x.split())\n",
    ")\n",
    "\n",
    "# Split narratives\n",
    "print(\"Chunking narratives...\")\n",
    "chunks = []\n",
    "for idx, row in df.iterrows():\n",
    "    splits = splitter.split_text(row['Consumer complaint narrative'])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            'complaint_id': row['Complaint ID'],\n",
    "            'product': row['Product'],\n",
    "            'chunk': split\n",
    "        })\n",
    "\n",
    "# Save chunks\n",
    "df_chunks = pd.DataFrame(chunks)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "df_chunks.to_csv(chunks_file, index=False)\n",
    "print(f\"âœ… Created {len(df_chunks):,} chunks, saved to {chunks_file}\")\n",
    "\n",
    "# Display chunk statistics\n",
    "chunk_lengths = df_chunks['chunk'].str.split().str.len()\n",
    "print(f\"\\nðŸ“Š Chunk Statistics:\")\n",
    "print(f\"Total chunks: {len(df_chunks):,}\")\n",
    "print(f\"Unique complaints: {df_chunks['complaint_id'].nunique():,}\")\n",
    "print(f\"Average words per chunk: {chunk_lengths.mean():.1f}\")\n",
    "print(f\"Median words per chunk: {chunk_lengths.median():.1f}\")\n",
    "print(f\"Products: {df_chunks['product'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Python\\creditrust-complaint-analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing embedding model...\n",
      "âœ… Model loaded: 384 dimensions\n",
      "\n",
      "ðŸ§ª Testing embedding on sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample embeddings shape: (5, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load chunks if not already loaded\n",
    "if 'df_chunks' not in locals():\n",
    "    df_chunks = pd.read_csv(chunks_file)\n",
    "\n",
    "# Initialize model\n",
    "print(\"ðŸ”§ Initializing embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ… Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")\n",
    "\n",
    "# Test embedding on a small sample first\n",
    "print(\"\\nðŸ§ª Testing embedding on sample...\")\n",
    "sample_texts = df_chunks['chunk'].head(5).tolist()\n",
    "sample_embeddings = model.encode(sample_texts, batch_size=32, show_progress_bar=True)\n",
    "print(f\"âœ… Sample embeddings shape: {sample_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating embeddings for all chunks...\n",
      "This may take 15-45 minutes depending on your system...\n",
      "Processing batch 1/393 (1-1,000 chunks)...\n",
      "Processing batch 2/393 (1001-2,000 chunks)...\n",
      "Processing batch 3/393 (2001-3,000 chunks)...\n",
      "Processing batch 4/393 (3001-4,000 chunks)...\n",
      "Processing batch 5/393 (4001-5,000 chunks)...\n",
      "Processing batch 6/393 (5001-6,000 chunks)...\n",
      "Processing batch 7/393 (6001-7,000 chunks)...\n",
      "Processing batch 8/393 (7001-8,000 chunks)...\n",
      "Processing batch 9/393 (8001-9,000 chunks)...\n",
      "Processing batch 10/393 (9001-10,000 chunks)...\n",
      "Processing batch 11/393 (10001-11,000 chunks)...\n",
      "Processing batch 12/393 (11001-12,000 chunks)...\n",
      "Processing batch 13/393 (12001-13,000 chunks)...\n",
      "Processing batch 14/393 (13001-14,000 chunks)...\n",
      "Processing batch 15/393 (14001-15,000 chunks)...\n",
      "Processing batch 16/393 (15001-16,000 chunks)...\n",
      "Processing batch 17/393 (16001-17,000 chunks)...\n",
      "Processing batch 18/393 (17001-18,000 chunks)...\n",
      "Processing batch 19/393 (18001-19,000 chunks)...\n",
      "Processing batch 20/393 (19001-20,000 chunks)...\n",
      "Processing batch 21/393 (20001-21,000 chunks)...\n",
      "Processing batch 22/393 (21001-22,000 chunks)...\n",
      "Processing batch 23/393 (22001-23,000 chunks)...\n",
      "Processing batch 24/393 (23001-24,000 chunks)...\n",
      "Processing batch 25/393 (24001-25,000 chunks)...\n",
      "Processing batch 26/393 (25001-26,000 chunks)...\n",
      "Processing batch 27/393 (26001-27,000 chunks)...\n",
      "Processing batch 28/393 (27001-28,000 chunks)...\n",
      "Processing batch 29/393 (28001-29,000 chunks)...\n",
      "Processing batch 30/393 (29001-30,000 chunks)...\n",
      "Processing batch 31/393 (30001-31,000 chunks)...\n",
      "Processing batch 32/393 (31001-32,000 chunks)...\n",
      "Processing batch 33/393 (32001-33,000 chunks)...\n",
      "Processing batch 34/393 (33001-34,000 chunks)...\n",
      "Processing batch 35/393 (34001-35,000 chunks)...\n",
      "Processing batch 36/393 (35001-36,000 chunks)...\n",
      "Processing batch 37/393 (36001-37,000 chunks)...\n",
      "Processing batch 38/393 (37001-38,000 chunks)...\n",
      "Processing batch 39/393 (38001-39,000 chunks)...\n",
      "Processing batch 40/393 (39001-40,000 chunks)...\n",
      "Processing batch 41/393 (40001-41,000 chunks)...\n",
      "Processing batch 42/393 (41001-42,000 chunks)...\n",
      "Processing batch 43/393 (42001-43,000 chunks)...\n",
      "Processing batch 44/393 (43001-44,000 chunks)...\n",
      "Processing batch 45/393 (44001-45,000 chunks)...\n",
      "Processing batch 46/393 (45001-46,000 chunks)...\n",
      "Processing batch 47/393 (46001-47,000 chunks)...\n",
      "Processing batch 48/393 (47001-48,000 chunks)...\n",
      "Processing batch 49/393 (48001-49,000 chunks)...\n",
      "Processing batch 50/393 (49001-50,000 chunks)...\n",
      "Processing batch 51/393 (50001-51,000 chunks)...\n",
      "Processing batch 52/393 (51001-52,000 chunks)...\n",
      "Processing batch 53/393 (52001-53,000 chunks)...\n",
      "Processing batch 54/393 (53001-54,000 chunks)...\n",
      "Processing batch 55/393 (54001-55,000 chunks)...\n",
      "Processing batch 56/393 (55001-56,000 chunks)...\n",
      "Processing batch 57/393 (56001-57,000 chunks)...\n",
      "Processing batch 58/393 (57001-58,000 chunks)...\n",
      "Processing batch 59/393 (58001-59,000 chunks)...\n",
      "Processing batch 60/393 (59001-60,000 chunks)...\n",
      "Processing batch 61/393 (60001-61,000 chunks)...\n",
      "Processing batch 62/393 (61001-62,000 chunks)...\n",
      "Processing batch 63/393 (62001-63,000 chunks)...\n",
      "Processing batch 64/393 (63001-64,000 chunks)...\n",
      "Processing batch 65/393 (64001-65,000 chunks)...\n",
      "Processing batch 66/393 (65001-66,000 chunks)...\n",
      "Processing batch 67/393 (66001-67,000 chunks)...\n",
      "Processing batch 68/393 (67001-68,000 chunks)...\n",
      "Processing batch 69/393 (68001-69,000 chunks)...\n",
      "Processing batch 70/393 (69001-70,000 chunks)...\n",
      "Processing batch 71/393 (70001-71,000 chunks)...\n",
      "Processing batch 72/393 (71001-72,000 chunks)...\n",
      "Processing batch 73/393 (72001-73,000 chunks)...\n",
      "Processing batch 74/393 (73001-74,000 chunks)...\n",
      "Processing batch 75/393 (74001-75,000 chunks)...\n",
      "Processing batch 76/393 (75001-76,000 chunks)...\n",
      "Processing batch 77/393 (76001-77,000 chunks)...\n",
      "Processing batch 78/393 (77001-78,000 chunks)...\n",
      "Processing batch 79/393 (78001-79,000 chunks)...\n",
      "Processing batch 80/393 (79001-80,000 chunks)...\n",
      "Processing batch 81/393 (80001-81,000 chunks)...\n",
      "Processing batch 82/393 (81001-82,000 chunks)...\n",
      "Processing batch 83/393 (82001-83,000 chunks)...\n",
      "Processing batch 84/393 (83001-84,000 chunks)...\n",
      "Processing batch 85/393 (84001-85,000 chunks)...\n",
      "Processing batch 86/393 (85001-86,000 chunks)...\n",
      "Processing batch 87/393 (86001-87,000 chunks)...\n",
      "Processing batch 88/393 (87001-88,000 chunks)...\n",
      "Processing batch 89/393 (88001-89,000 chunks)...\n",
      "Processing batch 90/393 (89001-90,000 chunks)...\n",
      "Processing batch 91/393 (90001-91,000 chunks)...\n",
      "Processing batch 92/393 (91001-92,000 chunks)...\n",
      "Processing batch 93/393 (92001-93,000 chunks)...\n",
      "Processing batch 94/393 (93001-94,000 chunks)...\n",
      "Processing batch 95/393 (94001-95,000 chunks)...\n",
      "Processing batch 96/393 (95001-96,000 chunks)...\n",
      "Processing batch 97/393 (96001-97,000 chunks)...\n",
      "Processing batch 98/393 (97001-98,000 chunks)...\n",
      "Processing batch 99/393 (98001-99,000 chunks)...\n",
      "Processing batch 100/393 (99001-100,000 chunks)...\n",
      "Processing batch 101/393 (100001-101,000 chunks)...\n",
      "Processing batch 102/393 (101001-102,000 chunks)...\n",
      "Processing batch 103/393 (102001-103,000 chunks)...\n",
      "Processing batch 104/393 (103001-104,000 chunks)...\n",
      "Processing batch 105/393 (104001-105,000 chunks)...\n",
      "Processing batch 106/393 (105001-106,000 chunks)...\n",
      "Processing batch 107/393 (106001-107,000 chunks)...\n",
      "Processing batch 108/393 (107001-108,000 chunks)...\n",
      "Processing batch 109/393 (108001-109,000 chunks)...\n",
      "Processing batch 110/393 (109001-110,000 chunks)...\n",
      "Processing batch 111/393 (110001-111,000 chunks)...\n",
      "Processing batch 112/393 (111001-112,000 chunks)...\n",
      "Processing batch 113/393 (112001-113,000 chunks)...\n",
      "Processing batch 114/393 (113001-114,000 chunks)...\n",
      "Processing batch 115/393 (114001-115,000 chunks)...\n",
      "Processing batch 116/393 (115001-116,000 chunks)...\n",
      "Processing batch 117/393 (116001-117,000 chunks)...\n",
      "Processing batch 118/393 (117001-118,000 chunks)...\n",
      "Processing batch 119/393 (118001-119,000 chunks)...\n",
      "Processing batch 120/393 (119001-120,000 chunks)...\n",
      "Processing batch 121/393 (120001-121,000 chunks)...\n",
      "Processing batch 122/393 (121001-122,000 chunks)...\n",
      "Processing batch 123/393 (122001-123,000 chunks)...\n",
      "Processing batch 124/393 (123001-124,000 chunks)...\n",
      "Processing batch 125/393 (124001-125,000 chunks)...\n",
      "Processing batch 126/393 (125001-126,000 chunks)...\n",
      "Processing batch 127/393 (126001-127,000 chunks)...\n",
      "Processing batch 128/393 (127001-128,000 chunks)...\n",
      "Processing batch 129/393 (128001-129,000 chunks)...\n",
      "Processing batch 130/393 (129001-130,000 chunks)...\n",
      "Processing batch 131/393 (130001-131,000 chunks)...\n",
      "Processing batch 132/393 (131001-132,000 chunks)...\n",
      "Processing batch 133/393 (132001-133,000 chunks)...\n",
      "Processing batch 134/393 (133001-134,000 chunks)...\n",
      "Processing batch 135/393 (134001-135,000 chunks)...\n",
      "Processing batch 136/393 (135001-136,000 chunks)...\n",
      "Processing batch 137/393 (136001-137,000 chunks)...\n",
      "Processing batch 138/393 (137001-138,000 chunks)...\n",
      "Processing batch 139/393 (138001-139,000 chunks)...\n",
      "Processing batch 140/393 (139001-140,000 chunks)...\n",
      "Processing batch 141/393 (140001-141,000 chunks)...\n",
      "Processing batch 142/393 (141001-142,000 chunks)...\n",
      "Processing batch 143/393 (142001-143,000 chunks)...\n",
      "Processing batch 144/393 (143001-144,000 chunks)...\n",
      "Processing batch 145/393 (144001-145,000 chunks)...\n",
      "Processing batch 146/393 (145001-146,000 chunks)...\n",
      "Processing batch 147/393 (146001-147,000 chunks)...\n",
      "Processing batch 148/393 (147001-148,000 chunks)...\n",
      "Processing batch 149/393 (148001-149,000 chunks)...\n",
      "Processing batch 150/393 (149001-150,000 chunks)...\n",
      "Processing batch 151/393 (150001-151,000 chunks)...\n",
      "Processing batch 152/393 (151001-152,000 chunks)...\n",
      "Processing batch 153/393 (152001-153,000 chunks)...\n",
      "Processing batch 154/393 (153001-154,000 chunks)...\n",
      "Processing batch 155/393 (154001-155,000 chunks)...\n",
      "Processing batch 156/393 (155001-156,000 chunks)...\n",
      "Processing batch 157/393 (156001-157,000 chunks)...\n",
      "Processing batch 158/393 (157001-158,000 chunks)...\n",
      "Processing batch 159/393 (158001-159,000 chunks)...\n",
      "Processing batch 160/393 (159001-160,000 chunks)...\n",
      "Processing batch 161/393 (160001-161,000 chunks)...\n",
      "Processing batch 162/393 (161001-162,000 chunks)...\n",
      "Processing batch 163/393 (162001-163,000 chunks)...\n",
      "Processing batch 164/393 (163001-164,000 chunks)...\n",
      "Processing batch 165/393 (164001-165,000 chunks)...\n",
      "Processing batch 166/393 (165001-166,000 chunks)...\n",
      "Processing batch 167/393 (166001-167,000 chunks)...\n",
      "Processing batch 168/393 (167001-168,000 chunks)...\n",
      "Processing batch 169/393 (168001-169,000 chunks)...\n",
      "Processing batch 170/393 (169001-170,000 chunks)...\n",
      "Processing batch 171/393 (170001-171,000 chunks)...\n",
      "Processing batch 172/393 (171001-172,000 chunks)...\n",
      "Processing batch 173/393 (172001-173,000 chunks)...\n",
      "Processing batch 174/393 (173001-174,000 chunks)...\n",
      "Processing batch 175/393 (174001-175,000 chunks)...\n",
      "Processing batch 176/393 (175001-176,000 chunks)...\n",
      "Processing batch 177/393 (176001-177,000 chunks)...\n",
      "Processing batch 178/393 (177001-178,000 chunks)...\n",
      "Processing batch 179/393 (178001-179,000 chunks)...\n",
      "Processing batch 180/393 (179001-180,000 chunks)...\n",
      "Processing batch 181/393 (180001-181,000 chunks)...\n",
      "Processing batch 182/393 (181001-182,000 chunks)...\n",
      "Processing batch 183/393 (182001-183,000 chunks)...\n",
      "Processing batch 184/393 (183001-184,000 chunks)...\n",
      "Processing batch 185/393 (184001-185,000 chunks)...\n",
      "Processing batch 186/393 (185001-186,000 chunks)...\n",
      "Processing batch 187/393 (186001-187,000 chunks)...\n",
      "Processing batch 188/393 (187001-188,000 chunks)...\n",
      "Processing batch 189/393 (188001-189,000 chunks)...\n",
      "Processing batch 190/393 (189001-190,000 chunks)...\n",
      "Processing batch 191/393 (190001-191,000 chunks)...\n",
      "Processing batch 192/393 (191001-192,000 chunks)...\n",
      "Processing batch 193/393 (192001-193,000 chunks)...\n",
      "Processing batch 194/393 (193001-194,000 chunks)...\n",
      "Processing batch 195/393 (194001-195,000 chunks)...\n",
      "Processing batch 196/393 (195001-196,000 chunks)...\n",
      "Processing batch 197/393 (196001-197,000 chunks)...\n",
      "Processing batch 198/393 (197001-198,000 chunks)...\n",
      "Processing batch 199/393 (198001-199,000 chunks)...\n",
      "Processing batch 200/393 (199001-200,000 chunks)...\n",
      "Processing batch 201/393 (200001-201,000 chunks)...\n",
      "Processing batch 202/393 (201001-202,000 chunks)...\n",
      "Processing batch 203/393 (202001-203,000 chunks)...\n",
      "Processing batch 204/393 (203001-204,000 chunks)...\n",
      "Processing batch 205/393 (204001-205,000 chunks)...\n",
      "Processing batch 206/393 (205001-206,000 chunks)...\n",
      "Processing batch 207/393 (206001-207,000 chunks)...\n",
      "Processing batch 208/393 (207001-208,000 chunks)...\n",
      "Processing batch 209/393 (208001-209,000 chunks)...\n",
      "Processing batch 210/393 (209001-210,000 chunks)...\n",
      "Processing batch 211/393 (210001-211,000 chunks)...\n",
      "Processing batch 212/393 (211001-212,000 chunks)...\n",
      "Processing batch 213/393 (212001-213,000 chunks)...\n",
      "Processing batch 214/393 (213001-214,000 chunks)...\n",
      "Processing batch 215/393 (214001-215,000 chunks)...\n",
      "Processing batch 216/393 (215001-216,000 chunks)...\n",
      "Processing batch 217/393 (216001-217,000 chunks)...\n",
      "Processing batch 218/393 (217001-218,000 chunks)...\n",
      "Processing batch 219/393 (218001-219,000 chunks)...\n",
      "Processing batch 220/393 (219001-220,000 chunks)...\n",
      "Processing batch 221/393 (220001-221,000 chunks)...\n",
      "Processing batch 222/393 (221001-222,000 chunks)...\n",
      "Processing batch 223/393 (222001-223,000 chunks)...\n",
      "Processing batch 224/393 (223001-224,000 chunks)...\n",
      "Processing batch 225/393 (224001-225,000 chunks)...\n",
      "Processing batch 226/393 (225001-226,000 chunks)...\n",
      "Processing batch 227/393 (226001-227,000 chunks)...\n",
      "Processing batch 228/393 (227001-228,000 chunks)...\n",
      "Processing batch 229/393 (228001-229,000 chunks)...\n",
      "Processing batch 230/393 (229001-230,000 chunks)...\n",
      "Processing batch 231/393 (230001-231,000 chunks)...\n",
      "Processing batch 232/393 (231001-232,000 chunks)...\n",
      "Processing batch 233/393 (232001-233,000 chunks)...\n",
      "Processing batch 234/393 (233001-234,000 chunks)...\n",
      "Processing batch 235/393 (234001-235,000 chunks)...\n",
      "Processing batch 236/393 (235001-236,000 chunks)...\n",
      "Processing batch 237/393 (236001-237,000 chunks)...\n",
      "Processing batch 238/393 (237001-238,000 chunks)...\n",
      "Processing batch 239/393 (238001-239,000 chunks)...\n",
      "Processing batch 240/393 (239001-240,000 chunks)...\n",
      "Processing batch 241/393 (240001-241,000 chunks)...\n",
      "Processing batch 242/393 (241001-242,000 chunks)...\n",
      "Processing batch 243/393 (242001-243,000 chunks)...\n",
      "Processing batch 244/393 (243001-244,000 chunks)...\n",
      "Processing batch 245/393 (244001-245,000 chunks)...\n",
      "Processing batch 246/393 (245001-246,000 chunks)...\n",
      "Processing batch 247/393 (246001-247,000 chunks)...\n",
      "Processing batch 248/393 (247001-248,000 chunks)...\n",
      "Processing batch 249/393 (248001-249,000 chunks)...\n",
      "Processing batch 250/393 (249001-250,000 chunks)...\n",
      "Processing batch 251/393 (250001-251,000 chunks)...\n",
      "Processing batch 252/393 (251001-252,000 chunks)...\n",
      "Processing batch 253/393 (252001-253,000 chunks)...\n",
      "Processing batch 254/393 (253001-254,000 chunks)...\n",
      "Processing batch 255/393 (254001-255,000 chunks)...\n",
      "Processing batch 256/393 (255001-256,000 chunks)...\n",
      "Processing batch 257/393 (256001-257,000 chunks)...\n",
      "Processing batch 258/393 (257001-258,000 chunks)...\n",
      "Processing batch 259/393 (258001-259,000 chunks)...\n",
      "Processing batch 260/393 (259001-260,000 chunks)...\n",
      "Processing batch 261/393 (260001-261,000 chunks)...\n",
      "Processing batch 262/393 (261001-262,000 chunks)...\n",
      "Processing batch 263/393 (262001-263,000 chunks)...\n",
      "Processing batch 264/393 (263001-264,000 chunks)...\n",
      "Processing batch 265/393 (264001-265,000 chunks)...\n",
      "Processing batch 266/393 (265001-266,000 chunks)...\n",
      "Processing batch 267/393 (266001-267,000 chunks)...\n",
      "Processing batch 268/393 (267001-268,000 chunks)...\n",
      "Processing batch 269/393 (268001-269,000 chunks)...\n",
      "Processing batch 270/393 (269001-270,000 chunks)...\n",
      "Processing batch 271/393 (270001-271,000 chunks)...\n",
      "Processing batch 272/393 (271001-272,000 chunks)...\n",
      "Processing batch 273/393 (272001-273,000 chunks)...\n",
      "Processing batch 274/393 (273001-274,000 chunks)...\n",
      "Processing batch 275/393 (274001-275,000 chunks)...\n",
      "Processing batch 276/393 (275001-276,000 chunks)...\n",
      "Processing batch 277/393 (276001-277,000 chunks)...\n",
      "Processing batch 278/393 (277001-278,000 chunks)...\n",
      "Processing batch 279/393 (278001-279,000 chunks)...\n",
      "Processing batch 280/393 (279001-280,000 chunks)...\n",
      "Processing batch 281/393 (280001-281,000 chunks)...\n",
      "Processing batch 282/393 (281001-282,000 chunks)...\n",
      "Processing batch 283/393 (282001-283,000 chunks)...\n",
      "Processing batch 284/393 (283001-284,000 chunks)...\n",
      "Processing batch 285/393 (284001-285,000 chunks)...\n",
      "Processing batch 286/393 (285001-286,000 chunks)...\n",
      "Processing batch 287/393 (286001-287,000 chunks)...\n",
      "Processing batch 288/393 (287001-288,000 chunks)...\n",
      "Processing batch 289/393 (288001-289,000 chunks)...\n",
      "Processing batch 290/393 (289001-290,000 chunks)...\n",
      "Processing batch 291/393 (290001-291,000 chunks)...\n",
      "Processing batch 292/393 (291001-292,000 chunks)...\n",
      "Processing batch 293/393 (292001-293,000 chunks)...\n",
      "Processing batch 294/393 (293001-294,000 chunks)...\n",
      "Processing batch 295/393 (294001-295,000 chunks)...\n",
      "Processing batch 296/393 (295001-296,000 chunks)...\n",
      "Processing batch 297/393 (296001-297,000 chunks)...\n",
      "Processing batch 298/393 (297001-298,000 chunks)...\n",
      "Processing batch 299/393 (298001-299,000 chunks)...\n",
      "Processing batch 300/393 (299001-300,000 chunks)...\n",
      "Processing batch 301/393 (300001-301,000 chunks)...\n",
      "Processing batch 302/393 (301001-302,000 chunks)...\n",
      "Processing batch 303/393 (302001-303,000 chunks)...\n",
      "Processing batch 304/393 (303001-304,000 chunks)...\n",
      "Processing batch 305/393 (304001-305,000 chunks)...\n",
      "Processing batch 306/393 (305001-306,000 chunks)...\n",
      "Processing batch 307/393 (306001-307,000 chunks)...\n",
      "Processing batch 308/393 (307001-308,000 chunks)...\n",
      "Processing batch 309/393 (308001-309,000 chunks)...\n",
      "Processing batch 310/393 (309001-310,000 chunks)...\n",
      "Processing batch 311/393 (310001-311,000 chunks)...\n",
      "Processing batch 312/393 (311001-312,000 chunks)...\n",
      "Processing batch 313/393 (312001-313,000 chunks)...\n",
      "Processing batch 314/393 (313001-314,000 chunks)...\n",
      "Processing batch 315/393 (314001-315,000 chunks)...\n",
      "Processing batch 316/393 (315001-316,000 chunks)...\n",
      "Processing batch 317/393 (316001-317,000 chunks)...\n",
      "Processing batch 318/393 (317001-318,000 chunks)...\n",
      "Processing batch 319/393 (318001-319,000 chunks)...\n",
      "Processing batch 320/393 (319001-320,000 chunks)...\n",
      "Processing batch 321/393 (320001-321,000 chunks)...\n",
      "Processing batch 322/393 (321001-322,000 chunks)...\n",
      "Processing batch 323/393 (322001-323,000 chunks)...\n",
      "Processing batch 324/393 (323001-324,000 chunks)...\n",
      "Processing batch 325/393 (324001-325,000 chunks)...\n",
      "Processing batch 326/393 (325001-326,000 chunks)...\n",
      "Processing batch 327/393 (326001-327,000 chunks)...\n",
      "Processing batch 328/393 (327001-328,000 chunks)...\n",
      "Processing batch 329/393 (328001-329,000 chunks)...\n",
      "Processing batch 330/393 (329001-330,000 chunks)...\n",
      "Processing batch 331/393 (330001-331,000 chunks)...\n",
      "Processing batch 332/393 (331001-332,000 chunks)...\n",
      "Processing batch 333/393 (332001-333,000 chunks)...\n",
      "Processing batch 334/393 (333001-334,000 chunks)...\n",
      "Processing batch 335/393 (334001-335,000 chunks)...\n",
      "Processing batch 336/393 (335001-336,000 chunks)...\n",
      "Processing batch 337/393 (336001-337,000 chunks)...\n",
      "Processing batch 338/393 (337001-338,000 chunks)...\n",
      "Processing batch 339/393 (338001-339,000 chunks)...\n",
      "Processing batch 340/393 (339001-340,000 chunks)...\n",
      "Processing batch 341/393 (340001-341,000 chunks)...\n",
      "Processing batch 342/393 (341001-342,000 chunks)...\n",
      "Processing batch 343/393 (342001-343,000 chunks)...\n",
      "Processing batch 344/393 (343001-344,000 chunks)...\n",
      "Processing batch 345/393 (344001-345,000 chunks)...\n",
      "Processing batch 346/393 (345001-346,000 chunks)...\n",
      "Processing batch 347/393 (346001-347,000 chunks)...\n",
      "Processing batch 348/393 (347001-348,000 chunks)...\n",
      "Processing batch 349/393 (348001-349,000 chunks)...\n",
      "Processing batch 350/393 (349001-350,000 chunks)...\n",
      "Processing batch 351/393 (350001-351,000 chunks)...\n",
      "Processing batch 352/393 (351001-352,000 chunks)...\n",
      "Processing batch 353/393 (352001-353,000 chunks)...\n",
      "Processing batch 354/393 (353001-354,000 chunks)...\n",
      "Processing batch 355/393 (354001-355,000 chunks)...\n",
      "Processing batch 356/393 (355001-356,000 chunks)...\n",
      "Processing batch 357/393 (356001-357,000 chunks)...\n",
      "Processing batch 358/393 (357001-358,000 chunks)...\n",
      "Processing batch 359/393 (358001-359,000 chunks)...\n",
      "Processing batch 360/393 (359001-360,000 chunks)...\n",
      "Processing batch 361/393 (360001-361,000 chunks)...\n",
      "Processing batch 362/393 (361001-362,000 chunks)...\n",
      "Processing batch 363/393 (362001-363,000 chunks)...\n",
      "Processing batch 364/393 (363001-364,000 chunks)...\n",
      "Processing batch 365/393 (364001-365,000 chunks)...\n",
      "Processing batch 366/393 (365001-366,000 chunks)...\n",
      "Processing batch 367/393 (366001-367,000 chunks)...\n",
      "Processing batch 368/393 (367001-368,000 chunks)...\n",
      "Processing batch 369/393 (368001-369,000 chunks)...\n",
      "Processing batch 370/393 (369001-370,000 chunks)...\n",
      "Processing batch 371/393 (370001-371,000 chunks)...\n",
      "Processing batch 372/393 (371001-372,000 chunks)...\n",
      "Processing batch 373/393 (372001-373,000 chunks)...\n",
      "Processing batch 374/393 (373001-374,000 chunks)...\n",
      "Processing batch 375/393 (374001-375,000 chunks)...\n",
      "Processing batch 376/393 (375001-376,000 chunks)...\n",
      "Processing batch 377/393 (376001-377,000 chunks)...\n",
      "Processing batch 378/393 (377001-378,000 chunks)...\n",
      "Processing batch 379/393 (378001-379,000 chunks)...\n",
      "Processing batch 380/393 (379001-380,000 chunks)...\n",
      "Processing batch 381/393 (380001-381,000 chunks)...\n",
      "Processing batch 382/393 (381001-382,000 chunks)...\n",
      "Processing batch 383/393 (382001-383,000 chunks)...\n",
      "Processing batch 384/393 (383001-384,000 chunks)...\n",
      "Processing batch 385/393 (384001-385,000 chunks)...\n",
      "Processing batch 386/393 (385001-386,000 chunks)...\n",
      "Processing batch 387/393 (386001-387,000 chunks)...\n",
      "Processing batch 388/393 (387001-388,000 chunks)...\n",
      "Processing batch 389/393 (388001-389,000 chunks)...\n",
      "Processing batch 390/393 (389001-390,000 chunks)...\n",
      "Processing batch 391/393 (390001-391,000 chunks)...\n",
      "Processing batch 392/393 (391001-392,000 chunks)...\n",
      "Processing batch 393/393 (392001-392,406 chunks)...\n",
      "\n",
      "âœ… Generated embeddings in 7900.3 seconds\n",
      "Total embeddings: 392,406\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(\"ðŸš€ Generating embeddings for all chunks...\")\n",
    "print(\"This may take 15-45 minutes depending on your system...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 1000\n",
    "all_embeddings = []\n",
    "total_batches = (len(df_chunks) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(0, len(df_chunks), batch_size):\n",
    "    batch_num = (i // batch_size) + 1\n",
    "    batch = df_chunks['chunk'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    print(f\"Processing batch {batch_num}/{total_batches} ({i+1}-{min(i+batch_size, len(df_chunks)):,} chunks)...\")\n",
    "    \n",
    "    batch_embeddings = model.encode(batch, batch_size=32, show_progress_bar=False)\n",
    "    all_embeddings.extend(batch_embeddings.tolist())\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Generated embeddings in {embedding_time:.1f} seconds\")\n",
    "print(f\"Total embeddings: {len(all_embeddings):,}\")\n",
    "print(f\"Embedding dimension: {len(all_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ï¸ Building FAISS vector store...\n",
      "Embeddings array shape: (392406, 384)\n",
      "Creating FAISS index with dimension: 384\n",
      "âœ… FAISS index created with 392,406 vectors\n",
      "Index dimension: 384\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"ðŸ—ï¸ Building FAISS vector store...\")\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_np = np.array(all_embeddings, dtype=np.float32)\n",
    "print(f\"Embeddings array shape: {embeddings_np.shape}\")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings_np.shape[1]  # Should be 384 for all-MiniLM-L6-v2\n",
    "print(f\"Creating FAISS index with dimension: {dimension}\")\n",
    "\n",
    "# Use IndexFlatL2 for exact L2 distance search\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"âœ… FAISS index created with {index.ntotal:,} vectors\")\n",
    "print(f\"Index dimension: {index.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Vector Store and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving FAISS index to ../vector_store\\faiss_index.bin...\n",
      "âœ… FAISS index saved (574.8 MB)\n",
      "ðŸ’¾ Saving metadata to ../vector_store\\metadata.csv...\n",
      "âœ… Metadata saved (398.1 MB)\n",
      "\n",
      "ðŸ“ Vector store files created in: ../vector_store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store directory\n",
    "vector_store_dir = '../vector_store'\n",
    "os.makedirs(vector_store_dir, exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "index_file = os.path.join(vector_store_dir, 'faiss_index.bin')\n",
    "print(f\"ðŸ’¾ Saving FAISS index to {index_file}...\")\n",
    "faiss.write_index(index, index_file)\n",
    "print(f\"âœ… FAISS index saved ({os.path.getsize(index_file) / (1024*1024):.1f} MB)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = os.path.join(vector_store_dir, 'metadata.csv')\n",
    "print(f\"ðŸ’¾ Saving metadata to {metadata_file}...\")\n",
    "metadata_df = df_chunks[['complaint_id', 'product', 'chunk']].copy()\n",
    "metadata_df.to_csv(metadata_file, index=False)\n",
    "print(f\"âœ… Metadata saved ({os.path.getsize(metadata_file) / (1024*1024):.1f} MB)\")\n",
    "\n",
    "print(f\"\\nðŸ“ Vector store files created in: {vector_store_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying vector store...\n",
      "Loading FAISS index...\n",
      "âœ… FAISS index loaded: 392,406 vectors, dimension 384\n",
      "Loading metadata...\n",
      "âœ… Metadata loaded: 392,406 rows\n",
      "Metadata columns: ['complaint_id', 'product', 'chunk']\n",
      "\n",
      "ðŸ§ª Testing similarity search...\n",
      "Test question: What are common credit card issues?\n",
      "Retrieved 3 similar chunks:\n",
      "  1. [Checking or savings account] general issues with debit card...\n",
      "     Distance: 0.727\n",
      "  2. [Credit card or prepaid card] as usual was using credit card i had fine established credit and company  xxxx  put freeze on uses of card...\n",
      "     Distance: 0.780\n",
      "  3. [Credit card or prepaid card] citizens credit card i paid my credit card off having problems for the last two months with payments being made and not being posted interest late fees being charged to my account then charging intere...\n",
      "     Distance: 0.800\n",
      "\n",
      "ðŸŽ‰ Vector store verification successful!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Verifying vector store...\")\n",
    "\n",
    "# Test loading FAISS index\n",
    "print(\"Loading FAISS index...\")\n",
    "test_index = faiss.read_index(index_file)\n",
    "print(f\"âœ… FAISS index loaded: {test_index.ntotal:,} vectors, dimension {test_index.d}\")\n",
    "\n",
    "# Test loading metadata\n",
    "print(\"Loading metadata...\")\n",
    "test_metadata = pd.read_csv(metadata_file)\n",
    "print(f\"âœ… Metadata loaded: {len(test_metadata):,} rows\")\n",
    "print(f\"Metadata columns: {test_metadata.columns.tolist()}\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nðŸ§ª Testing similarity search...\")\n",
    "test_question = \"What are common credit card issues?\"\n",
    "test_embedding = model.encode([test_question])\n",
    "distances, indices = test_index.search(test_embedding, k=3)\n",
    "\n",
    "print(f\"Test question: {test_question}\")\n",
    "print(f\"Retrieved {len(indices[0])} similar chunks:\")\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "    chunk_text = test_metadata.iloc[idx]['chunk'][:200] + \"...\"\n",
    "    product = test_metadata.iloc[idx]['product']\n",
    "    print(f\"  {i+1}. [{product}] {chunk_text}\")\n",
    "    print(f\"     Distance: {distance:.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Vector store verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Similarity Search with Different Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing similarity search with different queries...\n",
      "\n",
      "ðŸ“ Query: Why are people unhappy with BNPL?\n",
      "Top 3 most similar complaint chunks:\n",
      "  1. [Checking or savings account] Complaint 9967256\n",
      "     Chunk: have with bmo again i have never had such a miserable experience with a financial in\n",
      "stitution at no point did anyone at bmo take ownership of my conc...\n",
      "     Distance: 1.0844\n",
      "  2. [Credit card or prepaid card] Complaint 6067188\n",
      "     Chunk: i have spent well over 17000 worth of my time on this matter but to me there is a huge principle at stake here from the feelings i have gone through m...\n",
      "     Distance: 1.1661\n",
      "  3. [Checking or savings account] Complaint 9044009\n",
      "     Chunk: as a customer i feel very disappointed in their unfairness and disrespectful treatment of me having me steadily reach out to no avail call and go in p...\n",
      "     Distance: 1.1814\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Query: What billing issues do customers report?\n",
      "Top 3 most similar complaint chunks:\n",
      "  1. [Credit card or prepaid card] Complaint 3992588\n",
      "     Chunk: billing statement wasnt received or wasnt received in a timely fashion...\n",
      "     Distance: 0.6915\n",
      "  2. [Credit card or prepaid card] Complaint 6104842\n",
      "     Chunk: billing and errors dispute inaccurate investigative consumer reporting...\n",
      "     Distance: 0.7206\n",
      "  3. [Credit card or prepaid card] Complaint 3400974\n",
      "     Chunk: dissatisfied with purchases billing errors late fees errors phones calls no valid responses from letters sent accessing credit files and submitting er...\n",
      "     Distance: 0.7448\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Query: What fraud-related complaints exist?\n",
      "Top 3 most similar complaint chunks:\n",
      "  1. [Checking or savings account] Complaint 4430259\n",
      "     Chunk: fraud complain...\n",
      "     Distance: 0.4127\n",
      "  2. [Credit card or prepaid card] Complaint 5135069\n",
      "     Chunk: victim of bad practices and fraud...\n",
      "     Distance: 0.5777\n",
      "  3. [Money transfer, virtual currency, or money service] Complaint 11979613\n",
      "     Chunk: second complaint fraud said not fraud despite this person was brought up the xxxxxxxx xxxxxxxx pd and now claims incompetence never asked of proof of ...\n",
      "     Distance: 0.6177\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Query: What customer service problems are mentioned?\n",
      "Top 3 most similar complaint chunks:\n",
      "  1. [Money transfer, virtual currency, or money service] Complaint 11676996\n",
      "     Chunk: issues with money and lack of customer service to get it resolved...\n",
      "     Distance: 0.5325\n",
      "  2. [Consumer Loan] Complaint 2424213\n",
      "     Chunk: inaccurate information as well as severe lack of customer service...\n",
      "     Distance: 0.5544\n",
      "  3. [Money transfer, virtual currency, or money service] Complaint 11651990\n",
      "     Chunk: poor customer service and misleading terms of service...\n",
      "     Distance: 0.5901\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search on the FAISS index\n",
    "print(\"ðŸ” Testing similarity search with different queries...\")\n",
    "\n",
    "# Example queries\n",
    "test_queries = [\n",
    "    \"Why are people unhappy with BNPL?\",\n",
    "    \"What billing issues do customers report?\",\n",
    "    \"What fraud-related complaints exist?\",\n",
    "    \"What customer service problems are mentioned?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search FAISS index\n",
    "    k = 3\n",
    "    distances, indices = test_index.search(np.array(query_embedding, dtype=np.float32), k)\n",
    "    \n",
    "    print(f\"Top {k} most similar complaint chunks:\")\n",
    "    \n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        row = test_metadata.iloc[idx]\n",
    "        print(f\"  {i+1}. [{row['product']}] Complaint {row['complaint_id']}\")\n",
    "        print(f\"     Chunk: {row['chunk'][:150]}...\")\n",
    "        print(f\"     Distance: {distances[0][i]:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Task 2 Summary and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Task 2 Completion Summary\n",
      "==================================================\n",
      "âœ… Chunks processed: 392,406\n",
      "âœ… Embeddings generated: 392,406\n",
      "âœ… Embedding dimension: 384\n",
      "âœ… FAISS vectors: 392,406\n",
      "âœ… Processing time: 7900.3 seconds\n",
      "âœ… Vector store size: 574.8 MB\n",
      "âœ… Metadata size: 398.1 MB\n",
      "\n",
      "ðŸŽ¯ Task 2 Status: COMPLETED\n",
      "You can now proceed with Task 3: RAG Core Logic and Evaluation\n",
      "\n",
      "ðŸ“ Vector store contents:\n",
      "  faiss_index.bin (574.8 MB)\n",
      "  metadata.csv (398.1 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Task 2 Completion Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Chunks processed: {len(df_chunks):,}\")\n",
    "print(f\"âœ… Embeddings generated: {len(all_embeddings):,}\")\n",
    "print(f\"âœ… Embedding dimension: {len(all_embeddings[0])}\")\n",
    "print(f\"âœ… FAISS vectors: {test_index.ntotal:,}\")\n",
    "print(f\"âœ… Processing time: {embedding_time:.1f} seconds\")\n",
    "print(f\"âœ… Vector store size: {os.path.getsize(index_file) / (1024*1024):.1f} MB\")\n",
    "print(f\"âœ… Metadata size: {os.path.getsize(metadata_file) / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Task 2 Status: COMPLETED\")\n",
    "print(\"You can now proceed with Task 3: RAG Core Logic and Evaluation\")\n",
    "\n",
    "# List all files in vector store\n",
    "print(f\"\\nðŸ“ Vector store contents:\")\n",
    "for file in os.listdir(vector_store_dir):\n",
    "    file_path = os.path.join(vector_store_dir, file)\n",
    "    size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "    print(f\"  {file} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Summary: Chunking, Embedding, and Vector Store\n",
    "\n",
    "**Chunking Strategy:**  \n",
    "- Used `langchain.text_splitter.RecursiveCharacterTextSplitter`  \n",
    "- `chunk_size=500` words, `chunk_overlap=50`  \n",
    "- Chosen based on EDA: balances context and avoids cutting off important information\n",
    "\n",
    "**Embedding Model:**  \n",
    "- Used `sentence-transformers/all-MiniLM-L6-v2`  \n",
    "- Chosen for its speed, small size, and strong performance on semantic similarity tasks  \n",
    "- 384-dimensional embeddings, suitable for large-scale retrieval\n",
    "\n",
    "**Vector Store:**  \n",
    "- Used FAISS `IndexFlatL2` for fast, exact similarity search  \n",
    "- Stored metadata (complaint_id, product, chunk) for traceability\n",
    "\n",
    "**Deliverables:**  \n",
    "- `vector_store/faiss_index.bin` (FAISS index)  \n",
    "- `vector_store/metadata.csv` (chunk metadata)  \n",
    "- Ready for RAG retrieval in Task 3\n",
    "\n",
    "**Justification:**  \n",
    "- The chunking strategy ensures each vector is semantically meaningful and not too long for the embedding model.  \n",
    "- The chosen embedding model is efficient and accurate for retrieval tasks.  \n",
    "- FAISS enables scalable, fast semantic search for thousands of complaints.\n",
    "\n",
    "**Performance Metrics:**  \n",
    "- Total chunks: ~392K  \n",
    "- Embedding dimension: 384  \n",
    "- Vector store size: ~200-300 MB  \n",
    "- Processing time: 15-45 minutes (system dependent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
